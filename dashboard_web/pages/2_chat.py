# frontend/app_llm_chat.py
import streamlit as st
import requests

# ğŸ”„ ìƒíƒœ ì €ì¥
if "messages" not in st.session_state:
    st.session_state["messages"] = []

st.title("ğŸ’¬ LLM Frontend Chat")

# ê¸°ì¡´ ì±„íŒ… ì¶œë ¥
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# ì…ë ¥
if prompt := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    print(f'prompt = {prompt}')
    with st.chat_message("user"):
        st.markdown(prompt)

    # ğŸ’¬ Backend API í˜¸ì¶œ
    backend_url = "http://3.35.136.196:7002/predict/agent"  # ë°°í¬ ì‹œ IP ë³€ê²½
    response = requests.post(backend_url, json={"messages": st.session_state.messages})
    assistant_reply = response.json()["response"]

    st.session_state.messages.append({"role": "assistant", "content": assistant_reply})
    with st.chat_message("assistant"):
        st.markdown(assistant_reply)